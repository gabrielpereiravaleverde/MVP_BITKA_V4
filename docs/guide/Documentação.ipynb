{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Quality Banner](Images/banner_data_quality.png)\n",
    "\n",
    "\n",
    "**Projeto:** Otmização da Dosagem de Reagentes na Flotação.\\\n",
    "**Localização:** Municípios de Craíbas e Arapiraca, Estado de Alagoas.\\\n",
    "**Metodologia:** CRISP-DM.\\\n",
    "**Fase da Metodologia:** Deployment\\\n",
    "**Entregável:** Documentação do Pipeline MLOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Índice**<a id='toc0_'></a>    \n",
    "1. [Primeiros Passos](#section_1)\n",
    "    - 1.1 [Estrutura do projeto](#section_11)\n",
    "    - 1.2 [Elementos do framework de MLOps Kedro](#section_12)\n",
    "        - [Nodes](#nodes)\n",
    "        - [Pipelines](#pipelines)\n",
    "        - [Data Catalog](#data_catalog)\n",
    "    - 1.3 [Set Up do projeto](#section_13)\n",
    "        - [Criação de um ambiente virtual via conda](#section_14)\n",
    "        - [Instalação das dependências do projeto](#section_15)\n",
    "2. [Pipelines](#section_2)\n",
    "    - 2.1 [check_raw_data](#section_21)\n",
    "    - 2.2 [data_processing](#section_22)\n",
    "    - 2.3 [generate_models_input](#section_23)\n",
    "    - 2.4 [models_optimization](#section_24)\n",
    "    - 2.5 [data_science](#section_25)\n",
    "    - 2.5 [optimization](#section_26)\n",
    "3. [Fluxo do Projeto](#section_3)\n",
    "    - 3.1 [Processamento de novos dados](#section_31)\n",
    "    - 3.2 [Geração da Base Analítica do Modelo Preditivo](#section_32)\n",
    "    - 3.3 [Verificação da variável que captura mudanças na dinâmica do processo](#section_33)\n",
    "    - 3.4 [Otimização dos hyperparametros dos modelos preditivos](#section_34)\n",
    "    - 3.5 [Calibração dos modelos preditivos](#section_35)    \n",
    "    - 3.6 [Análise de erro/performance](#section_36)\n",
    "    - 3.7 [Simulação do Otimizador com o Modelo Preditivo](#section_37)\n",
    "    - 3.8 [Atualização do simulador MVP](#section_38)\n",
    "4. [Fluxo de Uso](#section_4)\n",
    "    - 4.1 [Processar Dados Novos](#section_41)\n",
    "    - 4.2 [Calibrar Modelo Baseline e Executar Simulação de Otimização](#section_42)\n",
    "    - 4.3 [Otimização de Hiperparâmetros do Modelo Campeão](#section_43)\n",
    "    - 4.4 [Calibrar Modelo Campeão e Executar Simulação de Otimização](#section_44)\n",
    "    - 4.5 [Comparar Resultados](#section_45)\n",
    "    - 4.6 [Atualizar Modelo no MVP (WebApp)](#section_46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='section_1'></a>[1. Primeiros Passos](#toc0_)  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta seção descreve a estrutura do projeto e como realizar sua configuração:\n",
    "\n",
    "- 1.1 [Estrutura do projeto](#section_11)\n",
    "- 1.2 [Elementos do framework de MLOps Kedro](#section_12)\n",
    "    - [Nodes](#nodes)\n",
    "    - [Pipelines](#pipelines)\n",
    "    - [Data Catalog](#data_catalog)\n",
    "- 1.3 [Set Up do projeto](#section_13)\n",
    "    - [Criação de um ambiente virtual via conda](#section_14)\n",
    "    - [Instalação das dependências do projeto](#section_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 <a id='section_11'></a>[ Estrutura do Projeto](#section_1)\n",
    "\n",
    "Esta seção descreve a estrutura de pastas e arquivos adotada no projeto. Este seguiu o padrão proposto pelo [framework de MLOps Kedro](https://kedro.readthedocs.io/en/stable/) com alguns arquivos adicionais ligados ao *web app MVP de Otimização da Flotação*. Essa organização visa proporcionar uma fácil manutenção, escalabilidade e colaboração durante o desenvolvimento.\n",
    "Recomenda-se a familiarização com esta estrutura para otimizar a eficiência durante a rotina de utilização, manutenção e evolução das componentes do projeto.\n",
    "\n",
    "<img src=\"Images/estrutura_de_pastas.png\" alt=\"Arquivos\" width=\"500\"/>\n",
    "\n",
    "\n",
    "##### **conf**\n",
    "\n",
    "A pasta `conf` contém todos os arquivos relacionados à configuração do projeto. Isso inclui o [Data Catalog](#data_catalog) e parâmetros de execução. Os arquivos presentes nesta pasta desempenham um papel fundamental na definição e personalização de aspectos operacionais do projeto.\n",
    "\n",
    "Exemplos de arquivos nesta pasta:\n",
    "\n",
    "- [`catalog.yml`](#data_catalog) : Descreve os datasets disponíveis e suas fontes.\n",
    "- `parameters.yml`: Armazena parâmetros de execução, facilitando a flexibilidade e reusabilidade do código.\n",
    "\n",
    "##### **data**\n",
    "\n",
    "A pasta `data` é dedicada ao armazenamento de datasets utilizados no projeto. Diferentes tipos de dados, como arquivos `.xlsx`, `.parquet` e `.json`, podem ser encontrados aqui.\n",
    "\n",
    "Estrutura de exemplo:\n",
    "\n",
    "```plaintext\n",
    "data/\n",
    "|-- raw/\n",
    "|   |-- dataset_raw.xlsx\n",
    "|-- processed/\n",
    "|   |-- dataset_processed.parquet\n",
    "|-- external/\n",
    "|   |-- external_data.json\n",
    "```\n",
    "##### **src**\n",
    "\n",
    "A pasta `src` contém todo o código fonte do projeto. As subpastas organizam o código de acordo com suas funcionalidades, tornando mais fácil a navegação e manutenção do código. Nela se encontram os códigos relativos aos [*pipelines*](#pipelines) e seus [*nodes*](#nodes). A descrição destes conceitos será realizado em uma outra seção deste documento (Ver [seção 1.2](#secao_12)).\n",
    "\n",
    "Exemplos de subpastas:\n",
    "\n",
    "- `pipelines/`: Contém scripts relacionados a pipelines de tratamento e modelagem preditiva.\n",
    "\n",
    "##### **docs**\n",
    "\n",
    "A pasta `docs` é destinada à documentação do projeto. Aqui, você encontrará informações detalhadas sobre o pipeline MLOps, instruções de uso e qualquer outra documentação relevante. O presente documento se encontra na pasta `/docs/guide`.\n",
    "\n",
    "##### **notebooks**\n",
    "\n",
    "A pasta `notebooks` abriga todos os Jupyter Notebooks utilizados durante o desenvolvimento do projeto. Esses notebooks podem incluir análises exploratórias, avaliação de resultados de modelos e outros experimentos. Servem como suporte à visualização e análise.\n",
    "\n",
    "##### **app**\n",
    "\n",
    "A pasta `app` contém o código fonte do *web app MVP de otimização da flotação* associado ao projeto. Esta pasta é específica ao projeto `mvvflotacao`, não compondo a estrutura padrão do [Kedro](#section_12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 <a id='section_12'></a>[Elementos do framework de MLOps Kedro](#section_1)\n",
    "\n",
    "\n",
    "\n",
    "[Kedro](https://kedro.readthedocs.io/en/stable/) é um framework Python de código aberto hospedada pela Linux Foundation (LF AI & Data). Kedro usa as melhores práticas de engenharia de software para ajudar equipes de Cientistas de Dados a construir códigos prontos para produção, diminuindo o esforço usual de refatoração de códigos experimentais.\n",
    "\n",
    "A utilização do framework busca um desenvolvimento de código modular mantendo-o organizado e seguindo boas práticas a medida que o projeto avança.\n",
    "\n",
    "Basicamente, os 3 principais [conceitos do Kedro](https://docs.kedro.org/en/stable/get_started/kedro_concepts.html#) são:\n",
    "- [Nodes](#nodes)\n",
    "- [Pipelines](#pipelines)\n",
    "- [Data Catalog](#data_catalog)\n",
    "\n",
    "Abordaremos cada um dos conceitos nesta seção. Mas recomendamos a [criação de um projeto básico a partir da documentação oficial do Kedro para um treinamento *hands-on*](https://docs.kedro.org/en/stable/tutorial/tutorial_template.html).\n",
    "\n",
    "Abaixo, observa-se um diagrama que ilustra a estrutura de um projeto kedro padrão (à esquerda) e uma simplificação da estrutura do projeto `mvvflotacao` desenvolvido neste projeto.\n",
    "\n",
    "<img src=\"Images/conceito_kedro_comparacao.png\" alt=\"Arquivos\" width=\"1200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='nodes'></a> [Nodes](#section_1)\n",
    "Um *node* (nó) no Kedro é um \"empacotador\"(*wrapper*) para uma função Python que nomeia as entradas e saídas dessa função. Os *nodes* são os blocos de construção de um *pipeline*, ou seja, um *pipeline* é um conjunto de *nodes*, que por sua vez representam funções em *python*.  Aqui está um exemplo simples de um *node* encontrado no *pipeline* `data_processing`, do projeto `mvvflotacao`:\n",
    "\n",
    "<img src=\"Images/node.png\" >\n",
    "\n",
    "O *node* acima empacota/declara a função `preprocess_reagentes` que receberá como *input* os objetos `reagentes_raw` (dataset de dados de dosagem de reagentes) e `params:raw_data_version` (parâmetro de versionamento de dados), ambos declarados no [*Data Catalog*](#data_catalog) do projeto. Como *output* a execução do *node* retornará o dataset `reagentes_pre` (dataset de dosagem de reagentes com tratamentos objetivando a modelagem preditiva). Por fim, todo *node* possui um nome único declarado pelo parâmetro *name*, neste caso, `reagentes_node`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='pipelines'></a> [Pipelines](#section_1)\n",
    "\n",
    "Um *pipeline* organiza as dependências e a ordem de execução de uma coleção de *nodes*. O *pipeline* determina a ordem de execução dos *nodes* resolvendo dependências e não necessariamente executa os *nodes* na ordem em que são passados. Como exemplo, segue abaixo uma parte do arquivo `pipeline.py` do *pipeline* `data_processing`:\n",
    "\n",
    "<img src=\"Images/codigo_pipeline.png\" alt=\"Arquivos\" width=\"500\"/>\n",
    "\n",
    "Observa-se no exemplo acima, a declaração de um conjunto de *nodes* (funções com seus respectivos objetos/arquivos de *input* e *output*) a serem executados no *pipeline* `data_processing`.\n",
    "\n",
    "Cada *pipeline* do projeto possui dois arquivos principais, o `node.py` que possui as funções escritas em python, e o arquivo `pipeline.py` que organiza os nodes conforme o exemplo anterior.\n",
    "\n",
    "<img src=\"Images/estrutura_pipeline.png\" alt=\"Arquivos\" width=\"150\"/>\n",
    "\n",
    "Os *nodes* são declarados no arquivo `pipeline.py` dentro de cada *pipeline* do projeto, referenciando funções escritas, em geral, nos arquivos `nodes.py`. \n",
    "\n",
    "Em casos onde a mesma função python é utilizada em mais de um *pipeline*, a mesma pode ser escrita fora do *pipeline* e importada diretamente no arquivo `pipeline.py`. Esta estratégia visa maior consistência geral do projeto. No projeto `mvvflotacao`, as funções compartilhadas estão localizadas em `src/mvvflotacao/shared_code`.\n",
    "\n",
    "\n",
    "\n",
    "Para criar um novo pipeline seguindo o template mencionado, pode-se utilizar a seguinte linha de código:\n",
    "\n",
    "```bash\n",
    "kedro pipeline create data_processing\n",
    "```\n",
    "\n",
    "Para executar um *pipeline* específico, deve-se utilizar o seguinte comando:\n",
    "\n",
    "```bash\n",
    "kedro run --pipeline=\"pipeline_name\"\n",
    "\n",
    "#exemplo para o pipeline data_processing\n",
    "kedro run --pipeline=data_processing\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar um *pipeline* específico, deve-se utilizar o comando abaixo. Porém lembre-se de verificar se todas as dependências (arquivos de *input* dos *nodes* do *pipeline a ser executado) já estão disponíveis.\n",
    "\n",
    "```bash\n",
    "kedro run --pipeline=\"pipeline_name\"\n",
    "\n",
    "#exemplo para o pipeline data_processing\n",
    "kedro run --pipeline=data_processing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='data_catalog'></a> [Data Catalog](#section_1)\n",
    "\n",
    "O *Data Catalog* do kedro é um registro de todas as fontes de dados utilizadas no projeto. Nele são registrados todos os arquivos de origem (dados brutos), assim como datasets e objetos intermediários criados ao longo de todo fluxo de processamento. O *Data Catalog* do kedro possui diversas funções *built-in* para leitura de arquivos que podem ser utilizados para ler arquivos *.xlsx* e *.csv*, por exemplo. Além disso, neste arquivo devem ser registrados todos os arquivos que serão utilizados como *input* e *output* dos *nodes* do projeto, sejam eles datasets (.parquet), modelos serializados (.pkl), hyperparametros (.json) ou qualquer outro formato utilizado.\n",
    "\n",
    "Desta forma, os arquivos de *nodes* especificados nos *pipelines* utilizarão como referência apenas os nomes dos arquivos especificados no *Data Catalog*, previamente configurados, não necessitando especificar seu *path* de forma explícita no código.\n",
    "\n",
    "Segue abaixo um trecho do arquivo *Data Catalog* do projeto `mvvflotacao` encontrado em `mvvflotacao/conf/base/catalog.yml`:\n",
    "\n",
    "<img src=\"Images/catalog.png\" alt=\"Arquivos\" width=\"400\"/> \n",
    "\n",
    "Observa-se no exemplo acima a especificação dos datasets `reagentes_raw` e `blend_raw` que estão armazedos no formato *.xlsx (ExcelDataset)*. Estes representam *dados brutos* recebidos diretamente dos bancos de dados da planta de flotação da MVV. Neste arquivo, encontra-se também a especificação dos datasets `cata_controle_pims_pre` e `balanco_de_massas_pre` armazeados em formato *.pq (ParquetDataset)*, um formato comumente utilizado em datasets intermediários de fluxos MLOps devido à sua velocidade de leitura e escrita.\n",
    "\n",
    "Observe agora o exemplo do *node* `reagentes_node`, encontrado em `./src/mvvflotacao/piepines/data_processing/pipeline.py`, responsável pelo pré-processamento inicial do dataset `reagentes_raw` (ligado à dosagem de reagentes). No exemplo, apenas os *inputs* e *outputs* são definidos seguindo os nomes registrados no *Data Catalog*. Com isso, não há a necessidade de especificar parâmetros de leitura e escritas adicionais.\n",
    "\n",
    "<img src=\"Images/node_example_data_catalog.png\" alt=\"Arquivos\" width=\"400\"/> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 <a id='section_13'></a>[ Set Up do projeto](#section_1)\n",
    "\n",
    "Para garantir a consistência do ambiente de desenvolvimento, recomenda-se a criação de um ambiente virtual específico para o projeto. Esta seção aborda o passo-a-passo encontrado na [Documentação Oficial do Kedro](https://docs.kedro.org/en/stable/get_started/install.html).\n",
    "\n",
    "##### **Instalação do Conda**\n",
    "\n",
    "Se você ainda não utiliza o Conda como gerenciador de ambientes virtuais, recomenda-se instalá-lo seguindo as instruções em [Conda Installation](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).\n",
    "\n",
    "#### <a id='section_14'></a>[Criação de um Novo Ambiente Virtual com Conda](#section_1)\n",
    "\n",
    "```bash\n",
    "# Substitua `my_kedro_env` pelo nome que deseja dar ao ambiente virtual\n",
    "conda create --name my_kedro_env python=3.10\n",
    "\n",
    "#Ative o ambiente virtual\n",
    "conda activate my_kedro_env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ao executar esses comandos, você terá criado um ambiente virtual isolado com o nome my_kedro_env e o Python 3.10 instalado. Certifique-se de ativar o ambiente virtual sempre que estiver trabalhando no projeto\n",
    "\n",
    "#### <a id='section_15'></a>[Instalação das Dependências do Projeto](#section_1)\n",
    "\n",
    "Certifique-se de estar na raiz do projeto antes de executar os comandos a seguir.\n",
    "\n",
    "```bash\n",
    "#Instale as dependências do projeto a partir do arquivo requirements.txt\n",
    "pip install -r src/requirements.txt\n",
    "\n",
    "# Ou inicialize pelo comando do Kedro\n",
    "kedro install \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ao seguir esses passos, você terá configurado um ambiente virtual com Conda e instalado as dependências do projeto, conforme as melhores práticas do Kedro. Certifique-se de manter o arquivo requirements.txt atualizado à medida que novas dependências forem adicionadas ao projeto. Isso garantirá consistência entre ambientes de desenvolvimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a id='section_2'></a>[Pipelines do Projeto `mvvflotacao`](#toc0_) [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O projeto `mvvflotacao` possui um conjunto de 6 *pipelines* principais encontrados em `./src/mvvflotacao/pipelines`, conforme a imagem abaixo:\n",
    "\n",
    "<img src=\"Images/pipelines_projeto_folders.png\" alt=\"Arquivos\" width=\"200\"/>\n",
    "\n",
    "Os *pipelines* compõem um grande fluxo que percorre as etapas de um projeto de *Advanced Analytics*, realizando desde à leitura de *dados brutos* recebidos diretamente das fontes oficias da MVV até a análise de otimização da dosagem de reagentes a partir do modelo preditivo treinado. \n",
    "\n",
    "Nesta seção realizaremos um resumo dos objetivos e fuções que compõem cada um dos  6 *pipelines* do projeto:\n",
    "\n",
    "- 2.1 [check_raw_data](#section_21)\n",
    "- 2.2 [data_processing](#section_22)\n",
    "- 2.3 [generate_models_input](#section_23)\n",
    "- 2.4 [models_optimization](#section_24)\n",
    "- 2.5 [data_science](#section_25)\n",
    "- 2.5 [optimization](#section_26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/pipelines_detalhados.png\" alt=\"Arquivos\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.1 <a id='section_21'></a> [ check_raw_data](#section_2)\n",
    "\n",
    "Este pipeline apresenta funções destinadas à verificar e comparar conjuntos de dados novos com conjuntos dados anteriores e verificar a existência de inconsistências.. Abaixo está um resumo das principais funcionalidades e lógicas presentes no código:\n",
    "\n",
    "##### Funções de Verificação\n",
    "\n",
    "`_check_column_names`:\n",
    "Compara os nomes de colunas entre dois dataframes e registra qualquer diferença encontrada.\n",
    "\n",
    "`_check_column_type`:\n",
    "Compara os tipos de dados das colunas entre dois dataframes e registra qualquer diferença encontrada.\n",
    "\n",
    "`_check_hist_values`:\n",
    "Compara as variações históricas nos dados entre dois dataframes e registra divergências.\n",
    "\n",
    "##### Função Principal `check_rawdata`\n",
    "\n",
    "- Recebe dois conjuntos de dados brutos, uma nova versão e uma versão anterior, e um nome de dataset.\n",
    "- Carrega os dataframes correspondentes às versões fornecidas.\n",
    "- Para o dataset 'reagentes', aplica o tratamento específico.\n",
    "- Realiza uma série de verificações utilizando as funções previamente definidas (`_check_column_names`, `_check_column_types`, `_check_hist_values`).\n",
    "- Registra mensagens informativas e de aviso conforme as diferenças ou divergências encontradas.\n",
    "\n",
    "O código visa garantir a consistência e qualidade dos dados ao longo do tempo, fornecendo informações detalhadas sobre qualquer alteração nos conjuntos de dados brutos. As verificações abrangem aspectos como nomes e tipos de colunas, bem como variações históricas, contribuindo para a manutenção da integridade dos dados no contexto do recebimento de novos dados.\n",
    "\n",
    "##### Utilização do Pipeline `check_raw_data`\n",
    "\n",
    "Para utilizar o pipeline `check_raw_data`, é necessário:\n",
    "\n",
    "-   Estrutura na nuvem ou localmente (referenciados no catálogo) com pastas contendo o nome respectivo ao nome da base, e arquivos referentes às datas (Os parâmetros do `check_raw_data` devem ser referentes ao nome dos arquivos de datas).\n",
    "-   Executar o pipeline com o comando `kedro run --pipeline=check_raw_data`.\n",
    "-   Garantir que os requisitos dos arquivos estruturados estejam atendidos.\n",
    "\n",
    "##### Análise dos Outputs\n",
    "\n",
    "Após a execução do pipeline, é importante analisar os outputs no terminal. Isso inclui verificar se as variáveis importantes das tabelas tiveram mudanças significativas, como alterações de nome das colunas, mudanças nos tipos de dados ou variações históricas inesperadas. Essas informações são cruciais para entender a integridade e a consistência dos dados recebidos.\n",
    "\n",
    "<img src=\"Images/exemplo_check_raw_data.png\" alt=\"Arquivos\" width=\"700\"/>\n",
    "\n",
    "- Na análise realizada, não foram identificadas alterações nos nomes e tipos das colunas. No entanto, foram observadas sutis variações nas datas nos últimos registros da base designada como \"previous_raw_data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  <a id='section_22'></a> [data_processing](#section_2)\n",
    "\n",
    "O código apresenta funções dedicadas ao pré-processamento de dados brutos, recebidos diretamente dos sistemas da planta de flotação da MVV. O *pipeline* visa ajustar o formato dos arquivos recebidos, ajustando estruturas como *data types*, formatos de data. Além disso, o *pipeline* realiza a preparação adequada dos dados ao treinamento de modelos preditivos, como a remoção de períodos inconsistentes e a agregações temporal. Abaixo está um resumo das principais funcionalidades e lógicas presentes no código:\n",
    "\n",
    "#####  **Função `preprocess_rawdata`**\n",
    "   - Converte colunas de tipo 'object' para 'string' em um DataFrame.\n",
    "   - Aceita um DataFrame ou um dicionário de funções para carregar DataFrames a partir de diferentes versões de dados brutos.\n",
    "\n",
    "##### **Funções de Pré-processamento Específicas para Dados de 'reagentes', 'laboratorio' e 'blend'**\n",
    "   - **`preprocess_reagentes`**\n",
    "      - Carrega e pré-processa dados específicos do dataset 'reagentes'.\n",
    "      - Realiza renomeação de colunas, manipulação de datas e horas, filtragem e preenchimento de valores faltantes.\n",
    "   - **`preprocess_laboratorio`**\n",
    "      - Carrega e pré-processa dados específicos do dataset 'laboratorio'.\n",
    "      - Realiza filtragem, manipulação de datas e horas, e cria cópias com ajustes temporais.\n",
    "   - **`preprocess_blend`**\n",
    "      - Carrega e pré-processa dados específicos do dataset 'blend'.\n",
    "      - Realiza combinação de colunas de data e hora, filtragem e seleção de colunas relevantes.\n",
    "\n",
    "#####  **Função `pivoting_blend`**\n",
    "   - Realiza a pivotação e pré-processamento dos dados de 'blend' para calcular a porcentagem de 'Tipo_Material' por hora.\n",
    "\n",
    "##### **Funções de Manipulação de Dados**\n",
    "   - **`filter_columns_before_merge`**\n",
    "      - Filtra colunas de um DataFrame baseado em uma tabela de metadados fornecida.\n",
    "   - **`merge_raw_data`**\n",
    "      - Mescla vários DataFrames usando a coluna 'DATA' como chave.\n",
    "   - **`remove_index_before_and_after`**\n",
    "      - Remove índices específicos e 'n' índices antes e depois de cada um.\n",
    "   - **`filter_data`**\n",
    "      -   Filtra um DataFrame removendo períodos específicos inconsistentes.\n",
    "   - **`agg_by_3h`:**\n",
    "      -   Agrega dados em intervalos de 3 horas, aplicando medidas estatísticas definidas nos parâmetros da pipeline.\n",
    "   - **`add_new_features`:**\n",
    "      -   Faz a média de variáveis similares para evitar problemas de colinearidade.\n",
    "      \n",
    "As funções do código visam garantir a consistência, tratamento e agregação apropriados dos dados brutos, fornecendo um conjunto de dados processado e pronto para análise preditiva.\n",
    "\n",
    "##### **Função de Divisão dos Dados**\n",
    "   - **`split_data`:**\n",
    "      -   Função para divisão dos dados em conjuntos de treinamento e teste, realiza essa divisão de maneira aleatória e também ordenando pela data.\n",
    "\n",
    "\n",
    "##### Execução do Pipeline `data_processing`\n",
    "\n",
    "Para executar este pipeline, utilize o comando:\n",
    "\n",
    "`kedro run --pipeline=data_processing`\n",
    "\n",
    "Este comando iniciará o processo de pré-processamento dos dados, aplicando todas as funções e lógicas descritas acima. É importante garantir que os dados brutos estejam corretamente formatados e disponíveis para que o pipeline funcione conforme esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 <a id='section_23'></a> [ generate_models_inputs](#section_2)\n",
    "\n",
    "Etapa de tratamento de dados prévia à utilização em técnicas de Machine Learning. Realiza tratamentos como feature engineering e remoção da autocorrelação nos dados. O objetivo deste *pipeline* é gerar os datasets utilizados como *input* direto no *pipeline* de treinamento dos modelos preditivos.\n",
    "\n",
    "##### **Limpeza de Dados e Registro de Avisos**\n",
    "\n",
    "- O código inclui seções dedicadas à limpeza de dados, como a remoção de colunas com uma porcentagem alta de valores nulos.\n",
    "\n",
    "#####  ***Feature Selection* e Pré-processamento de Dados**\n",
    "\n",
    "- **`select_features_from_step_columns`**: Seleciona colunas de recursos relevantes, considerando exclusões específicas e incluindo a coluna alvo.\n",
    "- **`get_target_name_for_model`**: Recupera o nome da coluna alvo para um modelo específico.\n",
    "- **`correct_type`**: Realiza a formatação adequada do DataFrame, definindo o índice, removendo colunas de data e convertendo tipos de colunas.\n",
    "- **`remove_autocorrelation`**: Remove a autocorrelação dos dados usando uma abordagem de amostragem sistemática.\n",
    "\n",
    "##### ***Feature Engineering***\n",
    "\n",
    "- **`create_lag_features`**: Gera características de defasagem para as colunas do DataFrame.\n",
    "- **`create_difflag_feature`**: Cria um novo DataFrame com características de diferença com base em colunas defasadas.\n",
    "- **`remove_outliers`**: Remove outliers dos conjuntos de treinamento e teste com base em determinadas colunas.\n",
    "\n",
    "##### **Parametrização de Medidas no Feature Engineering**\n",
    "\n",
    "-   No processo de *feature engineering*, as medidas são parametrizadas e podem ser selecionadas entre opções como mínimo (`min`), máximo (`max`), mediana (`median`), diferença entre mínimo e máximo (`diff_min_max`), e mediana (`median`). Isso permite uma flexibilidade na criação de features, adaptando-se às necessidades específicas de cada modelo.\n",
    "\n",
    "##### **Geração de Conjunto de Dados para Modelos Específicos**\n",
    "\n",
    "- Funções como `generate_dataset_for_conc_cd`, `generate_dataset_for_rej_rougher`, `generate_dataset_for_rec_global`, etc., geram conjuntos de dados específicos para modelos específicos com base em etapas e metadados fornecidos.\n",
    "\n",
    "Para executar este pipeline, utilize o comando:\n",
    "\n",
    "```bash\n",
    "kedro run --pipeline=generate_models_inputs\n",
    "```\n",
    "\n",
    "Este comando inicia o processo de geração de inputs para os modelos, aplicando todas as técnicas de pré-processamento, feature engineering e divisão de dados conforme descrito acima. É importante garantir que os parâmetros e os dados de entrada estejam corretamente configurados para que o pipeline funcione de maneira eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 <a id='section_24'></a> [ models_optimization](#section_2)\n",
    "\n",
    "Este pipeline é projetado para ajustar hiperparâmetros do modelo EBM e encontrar a combinação que resulta em menor RMSE médio a dentre amostras sistemáticas dos dados. \n",
    "\n",
    "As amostragens sistemáticas são realizadas em pipelines anteriores, onde em resumo, extrai-se 3 amostras dos dados completos intercalando sequencialmente às observações históricas às amostras. Exemplificando, o exemplo 1 (setup da flotação em um horário específico) nos dados de treino pertencerá à amostra 1, o exemplo 2 pertencerá à amostra 2, o exemplo 3 à amostra 3, o exemplo 4 percenterá à amostra 1, o exemplo 5 à amostra 2, assim por diante. Desta forma, geram-se 3 amostras similares para verificação da consistência dos aprendizados resultantes do treinamento de modelos preditivos.\n",
    "\n",
    "No processo de busca dos melhores hiperparâmetros para os modelos preditivos, neste caso, o `ExplainableBoostingRegressor` da biblioteca `interpret`, são performadas as etapas abaixo:\n",
    "\n",
    "##### **Otimização de Hiperparâmetros (`optimize_ebm_model`):**\n",
    "- Define um espaço de busca de hiperparâmetros para a otimização usando a biblioteca `hyperopt`.\n",
    "- Divide os dados em treinamento e teste para cada uma das três amostras sistemáticas.\n",
    "- Define uma função objetivo para a otimização que retorna a média dos RMSEs nas três amostras.\n",
    "- Realiza a otimização usando a estratégia de otimização Tree-structured Parzen Estimator (`tpe.suggest`).\n",
    "- Executa a otimização em iterações, salvando parcialmente os resultados a cada iteração.\n",
    "- O número de iterações é determinado pelos parâmetros do modelo (`params['max_iter']` e `params['save_iter']`).\n",
    "- Salva os resultados da otimização em um arquivo pickle para evitar recomeçar a otimização em caso de interrupção.\n",
    "- O resultado salvo é utilizado no *pipeline* de treinamento do modelo preditivo [`data_science`](#section_25).\n",
    "\n",
    "##### **Funções Auxiliares**\n",
    "\n",
    "**`split_data`**:\n",
    "- A função recebe um DataFrame completo (`full_data`) e o divide em conjuntos de treinamento e teste.\n",
    "- A divisão é feita ordenando os dados pela coluna 'DATA' e pegando os primeiros 80% para treinamento e o restante para teste.\n",
    "\n",
    "**`result_model`**:\n",
    "- Treina um modelo EBM nos dados de treinamento usando hiperparâmetros fornecidos.\n",
    "- Avalia o modelo treinado nos dados de teste usando a função `calc_rmse`.\n",
    "\n",
    "**`calc_rmse`**:\n",
    "- Calcula o Root Mean Squared Error (RMSE) das previsões do modelo nos dados de teste.\n",
    "\n",
    "\n",
    "##### **Notas Adicionais:**\n",
    "- O código suporta o carregamento parcial dos resultados da otimização para retomar de onde parou, se necessário.\n",
    "- Os resultados da otimização são salvos em um arquivo pickle para persistência entre execuções.\n",
    "\n",
    "Para executar toda a pipeline de `models_optimization`, deve-se rodar o seguinte código no terminal do anaconda:\n",
    "\n",
    "```bash\n",
    "kedro run --pipeline=models_optimization\n",
    "```\n",
    "\n",
    "Esta instrução, quando executada, iniciará o processo de otimização de modelo descrito na seção, rodando todas as etapas necessárias para encontrar os melhores hiperparâmetros para o modelo EBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 <a id='section_25'></a> [data_science](#section_2)\n",
    "\n",
    "O *pipeline* define funções para treinamento e avaliação dos resultados de modelos preditivos. As funções recebem como *input* os dados tratados nas etapas anteriores, assim como os parâmetros otimizados gerados no *pipeline* `models_optimization`. Como resultado, o *pipeline* retorna uma série de cursos para avaliação de performance, como gráficos e métricas, assim como o modelo treinado serializado.\n",
    "\n",
    "As principais funções incluem:\n",
    "\n",
    "\n",
    "##### **Treinamento do modelo preditivo**\n",
    "\n",
    "**`train_model`:** treina o modelo de machine learning flexível e parametrizado pelo arquivo de configuração. Atualmente, o pipeline suporta modelos do tipo ExplainableBoostingRegressor, uma implementação dos algoritmos de Boosting Machine porém com recursos que auxiliam em sua interpretabilidade. Pode-se treinar apenas um modelo ou um ensemble de EBMs. Novos modelos podem ser desenvolvidos para serem executados pelo pipeline (ver `src/mvvflotacao/shared_code/modeling/README.md`).\n",
    "\n",
    "##### **Previsão para avaliação dos Resultados**\n",
    "\n",
    "**`data_test_predict`:** Recebe um conjunto de dados de teste e um modelo treinado, faz previsões nos valores alvo e adiciona uma coluna 'Prediction' ao DataFrame.\n",
    "\n",
    "**`data_full_predict`:** Prevê valores alvo em um conjunto de dados completo usando um modelo treinado. Também realiza correção de tipo no DataFrame, convertendo colunas 'Float64' para 'float64' e removendo colunas 'datetime64[ns]'.\n",
    "\n",
    "##### **Métricas e Gráficos para Análise**\n",
    "\n",
    "**`calculate_metrics`:** Calcula métricas de avaliação (RMSE, MAE, R-squared) para um conjunto de dados de teste fornecido.\n",
    "\n",
    "**`create_model_snapshot`:** Cria um instantâneo das métricas e parâmetros de um modelo para documentação. Inclui informações como nome do modelo, nomes das features, comentários e vários parâmetros do modelo.\n",
    "\n",
    "**`consolidate_snapshots`:** Combina dados de várias partições (identificadas por chaves) em um único DataFrame. Utiliza funções fornecidas no dicionário 'partitions' para carregar dados de cada partição.\n",
    "\n",
    "**`create_valid_frame`:** Gera um DataFrame contendo dados de treinamento e teste, juntamente com previsões de teste. Renomeia a coluna 'predictions' para 'Prediction_Valid'.\n",
    "\n",
    "##### *Execução*\n",
    "\n",
    "Para executar todo o pipeline data_science, a instrução de comando é:\n",
    "\n",
    "```bash\n",
    "kedro run --pipeline=data_science\n",
    "```\n",
    "\n",
    "Este comando iniciará o processo descrito no pipeline data_science, realizando todas as etapas necessárias para treinar e gerar os arquivos para análise dos modelos preditivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 <a id='section_26'></a> [ optimization](#section_2)\n",
    "\n",
    "Executa análises de otimização de variáveis de decisão (ex: dosagem de reagentes e vazão de ar) usando um modelo treinado no *pipeline* `data_science`. Ele opera com base nos algorítimos de otimização gridsearch e otimização bayesiana.\n",
    "\n",
    "#####  Função de Simulação (`simulate`):\n",
    "- Gera cenários de simulação com base em uma instância da classe `Scenarios`.\n",
    "- Para cada cenário, realiza otimização utilizando um otimizador de busca em grade (`GridSearchOptimizer`) e um otimizador bayesiano (`BayesianOptimizer`).\n",
    "- Armazena os resultados das simulações.\n",
    "\n",
    "##### Processamento de Resultados (`process_results`):\n",
    "- Constrói um relatório de resultados usando a classe `SimulationResults` do módulo `optimizer_utils`.\n",
    "- Gera visualizações, incluindo gráficos de desempenho em relação a diferentes parâmetros.\n",
    "\n",
    "##### Otimizadores:\n",
    "- `GridSearchOptimizer`: Otimizador de busca em grade para avaliar todas as combinações possíveis.\n",
    "- `ItGridSearchOptimizer`: Versão iterativa do otimizador de busca em grade.\n",
    "- `BayesianOptimizer`: Otimizador bayesiano utilizando a biblioteca Optuna.\n",
    "- `GAOptimizer`: Otimizador genético (não implementado).\n",
    "\n",
    "##### *Execução*\n",
    "\n",
    "Para executar todo o pipeline optimization, a instrução de comando é:\n",
    "\n",
    "```bash\n",
    "kedro run --pipeline=optimization\n",
    "```\n",
    "\n",
    "- Este comando iniciará o processo de otimização descrito no pipeline optimization, onde serão exploradas diferentes combinações de variáveis de decisão para otimizar o desempenho do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 <a id='section_3'></a>[Fluxo do Projeto](#toc0_) [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O fluxo do projeto abrange diversas etapas, desde o processamento de novos dados até a atualização do Simulador MVP. Cada fase desempenha um papel crucial no desenvolvimento, calibração e otimização dos modelos preditivos. A seguir, detalhamos cada uma dessas etapas:\n",
    "\n",
    "  - 3.1 [Processamento de novos dados](#section_31)\n",
    "  - 3.2 [Geração da Base Analítica do Modelo Preditivo](#section_32)\n",
    "  - 3.3 [Verificação da variável que captura mudanças na dinâmica do processo](#section_33)\n",
    "  - 3.4 [Otimização dos hyperparametros dos modelos preditivos](#section_34)\n",
    "  - 3.5 [Calibração dos modelos preditivos](#section_35)    \n",
    "  - 3.6 [Análise de erro/performance](#section_36)\n",
    "  - 3.7 [Simulação do Otimizador com o Modelo Preditivo](#section_37)\n",
    "  - 3.8 [Atualização do simulador MVP](#section_38)\n",
    "\n",
    "\n",
    "#### 3.1  <a id='section_31'></a>[Processamento de Novos Dados](#section_3)\n",
    "\n",
    "Nesta fase, os dados recém-coletados ou atualizados são processados e integrados ao projeto. Isso inclui a validação, limpeza e transformação dos dados para garantir sua qualidade e utilidade nos modelos preditivos.\n",
    "\n",
    "Para essa etapa são utilizados os pipelines [`check_raw_data`](#section_21) e [`data_processing`](#section_22).\n",
    "\n",
    "Para atualizar os dados, é necessário alterar os os parâmetros de data no arquivo '\\conf\\base\\parameters.yml'.\n",
    "\n",
    "```yaml\n",
    "raw_data_version: '2023_12_18-07_00_00'\n",
    "previous_raw_data_version: '2023_12_10-17_00_00'\n",
    "models_to_solve: \n",
    "  - conc_cd\n",
    "  - rec_global\n",
    "```\n",
    "\n",
    "- raw_data_version: data/hora da nova base\n",
    "- previous_raw_data_version: data/hora anterior\n",
    "- models_to_solve: lista com os problemas que serão resolvidos por todos os pipelines. Neste exemplo, vemos que todas as execuções irão considerar os problemas de teor de cobre no CD e recuperação global. Caso o usuário queira executar os pipelines para somente um dos problemas, é só alterar este parâmetro.\n",
    "\n",
    "```yaml\n",
    "split_data:\n",
    "  test_size: 0.15\n",
    "\n",
    "measures: [\"min\", \"max\", \"median\", \"diff_min_max\"]\n",
    "```\n",
    "\n",
    "- split_data:\n",
    "    - test_size: porcentagem referente ao dataset de teste, os demais registros entrarão no dataset de treino \n",
    "- measures: medidas para agregar as observações dentro do intervalo de horas, por padrão é utilizada a média mas podem ser adicionadas outras medidas (opções: 'min','max','median','diff_min_max')\n",
    "\n",
    "**Armazenamento na Cloud**: \n",
    "\n",
    "Esse pipeline também suporta o armazenamento dos datasets calculados em buckets no Microsoft Azure Data Storage. Esse tipo de armazenamento facilita o compartilhamento dos outputs desse pipeline entre múltiplas pessoas. Nesse caso, é necessário que somente uma execução do pipeline data_processing seja feita e os resultados serão armazenados no bucket. A partir desse momento, as execuções dos demais pipelines utilizam os dados já pré-processados e salvos no bucket, economizando em tempo de processamento e também padronizando os resultados de execuções em múltiplos computadores.\n",
    "\n",
    "Para isso, é necessário alterar o catálogo de dados utilizados pelo Kedro em `conf/base/catalog.yml`. Um exemplo de catálogo com armazenamento no Data Storage pode ser visto em `conf/cloud/catalog.yml`. Um exemplo de entrada no arquivo para armezanamento em cloud é apresentado abaixo: \n",
    "\n",
    "```yaml\n",
    "balanco_de_massas_raw:\n",
    "  type: PartitionedDataset\n",
    "  path: abfs://mvvflotacao/01_raw/balanco_de_massas/\n",
    "  credentials: azure_data_storage\n",
    "  filename_suffix: .xlsx\n",
    "  dataset:\n",
    "    type: pandas.ExcelDataSet\n",
    "    load_args:\n",
    "      engine: openpyxl\n",
    "      decimal: \".\"\n",
    "      parse_dates: [\"DATA\"]\n",
    "```\n",
    "\n",
    "O atributo `path` deve refletir o endereço/bucket onde o dado está salvo.\n",
    "\n",
    "*Observação*: para garantir o acesso a bases armazenadas em serviços de nuvem, é necessário fornecer um arquivo de [credenciais](https://docs.kedro.org/en/stable/data/data_catalog.html) ao Kedro. Certifique-se de incluir esse arquivo para que o Kedro possa estabelecer a conexão necessária durante a execução do pipeline.\n",
    "\n",
    "**Armazenamento Local**:\n",
    "\n",
    "Para armazenamento local, as bases de dados devem ser adicionadas com o nome correspondente no arquivo 'catalog.yml'.\n",
    "\n",
    "```yaml\n",
    "laboratorio_raw:\n",
    "  type: pandas.ExcelDataSet\n",
    "  filepath: data/01_raw_data/laboratorio.xlsx\n",
    "  load_args:\n",
    "    engine: openpyxl\n",
    "    decimal: \".\"\n",
    "    parse_dates: [\"DATA\"]\n",
    "```\n",
    "\n",
    "Neste caso, o passo inicial para processar os dados seria adicioanr as bases na pasta ` data/01_raw_data/`, com os respectivos nomes no catálogo (nesse caso, laboratorio.xlsx).\n",
    "\n",
    "**Kedro Env:** \n",
    "\n",
    "O projeto é configurado para reconhecer o `catalog.yml` dentro de `conf\\base` como a configuração padrão. Se desejar utilizar outro catálogo, basta incluir o argumento `--env` seguido pelo nome da pasta que contém o respectivo `catalog.yml`.\n",
    "\n",
    "Para exemplificar, ao executar o pipeline completo de `data_processing` com armazenamento em nuvem, utilize o comando `kedro run --pipeline=data_processing --env=cloud`. Neste caso, a referência é feita à subpasta `cloud` dentro de `base`, onde o arquivo `catalog.yml` específico para o armazenamento em nuvem está localizado.\n",
    "\n",
    "#### 3.2  <a id='section_32'></a>[Geração da Base Analítica do Modelo Preditivo](#section_3)\n",
    "\n",
    "A etapa de geração das bases analíticas foi criada para determinar os inputs de cada modelo, nessa etapa deverá ser executado o pipeline [`generate_models_inputs`](#section_23). Nesse pipeline serão gerados os datasets utilizados para cada modelo, \n",
    "\n",
    "Caso seja necessário alterar as features utilizadas no modelo deve-se alterar o arquivo '\\conf\\base\\parameters_generate_models_inputs.yml'. Nesse arquivo, além de alterar as features, é possível alterar alguns parâmetros como a remoção de outliers do dataset de treino e/ou teste, remoção de valores nulos e zerados, criação de lags e seleção de agregações como mínimo, máximo, etc.\n",
    "\n",
    "Os parâmetros desta etapa estão detalhados a seguir:\n",
    "- **target:** nome da variável resposta (target)\n",
    "- **remove_outliers:** \n",
    "    - **train:** True/False, determina se deveram ser removidos os outliers do dataset de treino \n",
    "    - **test:** True/False, determina se deveram ser removidos os outliers do dataset de teste \n",
    "- **lag:** True/False, parâmetro para criação de variáveis em lag\n",
    "- **diff_lag:** True/False, parâmetro para criação de variáveis com as diferenças entre os lags 1 e 2\n",
    "- **filter_val_dardo_cd:** True/False, booleano para filtragem pelo campo de Válvula Dardo\n",
    "- **dropna:** True/False\n",
    "- **remove_0_values:** True/False, booleano para remoção das observações que possuem valor zero\n",
    "- **measures:** medidas para agregar as observações dentro do intervalo de horas, por padrão é utilizada a média mas podem ser adicionadas outras medidas (opções: 'min','max','median','diff_min_max')\n",
    "- **features:** lista com os nomes de todas as variáveis que serão utilizadas no modelo\n",
    "- **steps:** lista com todas as etapas que serão utilizadas no modelo\n",
    "- **dynamic_features:** períodos com mudança de comportamento (explicação detalhada na seção 3.3)\n",
    "\n",
    "Importante: \n",
    "\n",
    "Para gerar as bases pela lista de variáveis, a função do modelo nos nodes do generate_models_inputs deverá chamar a função _generate_dataset_by_params_ e utilizar o parâmetro _features_ conforme exemplo abaixo:\n",
    "\n",
    "```yaml\n",
    "generate_dataset_for_conc_cd:\n",
    "  target: \"CONC_ROUG_FC01_CUT\"\n",
    "  remove_outliers:\n",
    "    train: True\n",
    "    test: False\n",
    "  lag: False\n",
    "  diff_lag: False\n",
    "  filter_val_dardo_cd: True\n",
    "  dropna: True\n",
    "  remove_0_values: True\n",
    "  measures: ['min','max','median','diff_min_max']\n",
    "  features:\n",
    "    [\n",
    "      'DEPRESSOR_CD_LH',\n",
    "      'Sulfetado_MG_Estoque',\n",
    "      'VELOC_BF1_MOINHO',\n",
    "    ]\n",
    "  dynamic_feature: \n",
    "    [\n",
    "      '2023-07-03T03:00:00.000000000', \n",
    "      '2023-07-11T21:00:00.000000000',\n",
    "    ]\n",
    "```\n",
    "\n",
    "![nodes_generate_models_inputs_features](Images/nodes_generate_models_inputs_features.png)\n",
    "\n",
    "Para gerar as bases pelas etapas, a função de geração deverá chamar a função _generate_dataset_from_steps_ e utilizar o parâmetro _steps_ conforme exemplo abaixo:\n",
    "\n",
    "![nodes_generate_models_inputs_features](Images/nodes_generate_models_inputs_features.png)\n",
    "\n",
    "![parameters_generate_models_inputs_steps](Images/parameters_generate_models_inputs_steps.png)\n",
    "\n",
    "#### 3.3 <a id='section_33'></a>[Verificação da variável que captura mudanças na dinâmica do processo](#section_3)\n",
    "\n",
    "A variável de dinâmica do processo é utilizada nos modelos com intuito de trazer informações sobre a constante evolução dos processos produtivos na MVV, seja por mudanças da planta, insumos e/ou operacionais.\n",
    "\n",
    "Esta feature é calculada analisando a série temporal da variável target escolhida. No exemplo de teor de cobre no CD, tem-se o gráfico abaixo:\n",
    "\n",
    "<img src=\"Images/dynamic.png\" alt=\"Otimização\" width=\"1500\"/>\n",
    "\n",
    "Cada linha pontilhada se refere a alterações na dinâmica da variável de teor de cobre, ou seja, o algoritmo de offline change point detection considera que esta classificação minimiza as diferenças de comportamento em cada segmento. Ver o pacote [Ruptures](https://centre-borelli.github.io/ruptures-docs/) para mais informações.\n",
    "\n",
    "A execução desta análise pode ser feita com o Jupyter Notebook `notebooks/Dynamic Feature.ipynb`. As informações de mudança de dinâmica são passadas aos pipelines de geração de features via arquivo de configuração como uma lista de datas, ou seja, a lista de pontos onde ocorre uma mudança de dinâmica. Ver o arquivo de configuração `conf/base/parameters_generate_models_inputs.yml` para um exemplo.\n",
    "\n",
    "Caso sejam adquiridos mais dados, é importante re-executar essa análise para verificar se houve alguma mudança de dinâmica recente.\n",
    "\n",
    "#### 3.4 <a id='section_34'></a>[Otimização dos hyperparametros dos modelos preditivos](#section_3)\n",
    "\n",
    "Nesta fase, são realizados experimentos para encontrar os melhores hiperparâmetros dos modelos preditivos. Isso envolve a busca por combinações ideais que maximizem o desempenho do modelo. Para otimizar os hiperparâmetros, temos o pipeline [`models_optimization`](#section_24) onde é feita uma otimização bayesiana para encontrar os melhores hiperparâmetros para o modelo.\n",
    "\n",
    "Nos parâmetos da otimização de hiperparâmetros ('\\conf\\base\\parameters_models_optimization.yml'), temos:\n",
    "\n",
    "![parameters_models_optimization](Images/parameters_models_optimization.png)\n",
    "\n",
    "- load_trials: True/False, parâmetro para carregar a última tentativa ou começar uma nova otimização\n",
    "- max_iter: quantidade de tentativas (iterações) que a otimização vai realizar para encontrar os melhores hiperparâmetros\n",
    "- save_iter: parâmetro para salvar os resultados a cada x iterações, pois caso haja algum problema no meio da execução temos os resultados parciais\n",
    "\n",
    "Ao final do processo de otimização, o sistema salva automaticamente os hiperparâmetros que produziram os melhores resultados no arquivo padrão do modelo. Isso assegura que os avanços obtidos na otimização sejam preservados e facilmente acessíveis para uso futuro.\n",
    "\n",
    "#### 3.5 <a id='section_35'></a>[Calibração dos modelos preditivos](#section_3)\n",
    "\n",
    "A etapa de calibração do modelo é dedicada ao ajuste fino dos modelos preditivos. Os algoritmos são treinados utilizando conjuntos de dados históricos, refinando seus parâmetros para melhor se adaptarem às características específicas do sistema em questão. Essa etapa foi implementada no pipeline [`data_science`](#section_25).\n",
    "\n",
    "Parâmetros da calibração do modelo ('\\conf\\base\\parameters_data_science.yml'):\n",
    "\n",
    "![parameters_data_science](Images/parameters_data_science.png)\n",
    "\n",
    "- train_ebm_model:\n",
    "    - latest: True/False, para selecionar o último arquivo de hiperparâmetro salvo\n",
    "    - hyperparameters_path: caminho com o arquivo de hiperparâmetros que deve ser utilizado caso o parâmetro 'latest' seja falso\n",
    "- run_conformal_nodes: True/False\n",
    "\n",
    "- model:\n",
    "    - type: tipo do modelo que será treinado\n",
    "    - randomly_dataset_s0: True/False, booleano para treinar o modelo com o dataset dividido aleatoriamente\n",
    "    - by_date_dataset_s0: True/False, booleano para treinar o modelo com o dataset dividido por data\n",
    "    - features: lista com as features que serão utilizadas como input no modelo\n",
    "\n",
    "#### 3.6 <a id='section_36'></a>[Análise de erro/performance](#section_3)\n",
    "\n",
    "Ao final da execução dos pipelines `generate_models_input` e `data_science`, têm-se os modelos treinados e salvos na pasta `data/06_models`. Neste momento, é possível analisar os erros do modelo executando o Jupyter Notebook `notebooks/Modelagem-TeorCu-Cd new pipeline.ipynb`. A execução deste notebook gera como o relatório de métricas de performance para o modelo treinado. \n",
    "\n",
    "O relatório do modelo pode ser visto na figura abaixo. Neste relatório, mostram-se as métricas de performance do modelo, principalmente a Raiz do Erro Quadrático Médio (RMSE, em inglês) e o Coeficiente de Determinação R2 em duas abordagens de treinamento distintas: com split temporal e com split aleatório. \n",
    "\n",
    "<img src=\"Images/newplot.png\" alt=\"Otimização\" width=\"800\"/>\n",
    "\n",
    "No split temporal, utiliza-se o último mês de dados como teste e o restante como treino do modelo. Já no split aleatório, define-se que uma amostragem aleatória de 15% da base como teste e o restante é destinado ao treino. Reforça-se que o modelo destinado a ser utilizado em produção é sempre o com split aleatório, pois utiliza dados mais recentes para ser treinado, porém é interessante analisar o modelo por split temporal para entender as diferentes dinâmicas dos dados e do modelo ao longo do tempo.\n",
    "\n",
    "Por fim, o relatório acima mostra as 10 principais variáveis identificadas pelos modelos treinados nos diferentes paradigmas. \n",
    "\n",
    "**Determinação do Melhor Modelo:**\n",
    "\n",
    "No processo de escolha e determinação do modelo a ser utilizado em produção, recomenda-se as seguintes ações: \n",
    "- Analisar resultados baseline, ou seja, resultados que devem ser superados pelas modelagens mais complexas. Para isso, é necessário a execução do notebook `notebooks/Modelagem-TeorCu-Cd-Baseline.ipynb`. Nesse notebook, resultados como Lag 1 e média simples serão apresentados em termos das métricas de performance descritas anteriormente.\n",
    "- Gerar este mesmo relatório para modelos mais simples com menor conjunto de variáveis, ou seja, um modelo EBM simples com as variáveis mais importantes do ponto de vista de negócio. Um exemplo deste tipo de modelo mais simples pode ser treinado alterando o arquivo de configuração `conf/base/parameters_data_science.yml` para: \n",
    "\n",
    "```yaml\n",
    "model_conc_cd:\n",
    "  type: ebm\n",
    "  randomly_dataset_s0: True\n",
    "  by_date_dataset_s0: False\n",
    "  features:\n",
    "  [\n",
    "      'FLOT_AL_MASSA',\n",
    "      'P20_MOAGEM',\n",
    "      'P99_MOAGEM',\n",
    "      'PSI_OVER_CICLO',\n",
    "      'ALIM_FLOT_CU_TOT',\n",
    "      'ESPUMA_ROUGHER_COND',\n",
    "      'PH_ROUGHER_COND',\n",
    "      'VAL_DARDO_CD',\n",
    "      'VAZAO_AR_ROUGHER_COND',\n",
    "      'ALIM_FLOT_PER_SOL',\n",
    "      'ALIM_FLOT_FE',\n",
    "      'ALIM_FLOT_MG',\n",
    "      'ALIM_FLOT_NI',\n",
    "      'Espumante (g/t)_CD',\n",
    "      'CONC_ROUG_FC01_CUT',\n",
    "      'Sulfetado_LG',\n",
    "      'Sulfetado_HG',\n",
    "      'Sulfetado_MG',\n",
    "      'Sulfetado_SHG'\n",
    "  ]\n",
    "```\n",
    "\n",
    "- Avaliar os resultados do modelo anteriormente utilizado em produção para os dados mais recentes coletados. Por exemplo, considerando que o modelo utilizado atualmente foi treinado com dados até Novembro/23. Pode-se testar este mesmo modelo para os dados coletados em Dezembro/23. Este teste é comparável com um modelo treinado via split temporal até Novembro/23 e testado até Dezembro/23;\n",
    "- Comparar os resultados obtidos nos itens anteriores com o relatório extraído do notebook `notebooks/Modelagem-TeorCu-Cd new pipeline.ipynb`.\n",
    "\n",
    "\n",
    "#### 3.7 <a id='section_37'></a>[Simulação do Otimizador com o Modelo Preditivo](#section_3)\n",
    "\n",
    "O objetivo deste pipeline é testar o comportamento do modelo frente a otimização das variáveis de decisão escolhidas. \n",
    "\n",
    "Primeiramente, pode-se definir o problema de otimização como: $$\\underset{V_d}{\\text{argmax}} \\, \\mathcal{M}(V_d, X),$$ \n",
    "\n",
    "ou seja, deseja-se encontrar o melhor conjunto de variáveis de decisão $V_d$ que maximiza a resposta do Modelo $\\mathcal{M}$, considerando também outras variáveis de contexto $X$. Por exemplo, nos pipelines anteriores, define-se um modelo $\\hat{y} = \\mathcal{M}(V_d, X)$ que prevê o teor de cobre no CD $\\hat{y}$ dado um conjunto de features que é composto por $V_d$ e $X$.\n",
    "\n",
    "Para se avaliar comportamento do modelo na otimização, executa-se o otimizador para $n$ cenários distintos, ou seja, $n$ configurações diferentes de variáveis $X$. No pipeline desenvolvido, $n$ está definido como 30. A execução do pipeline pode ser configurada pelo arquivo de configuração no diretório `conf/base/parameters_optimization.yaml` descrito abaixo:\n",
    "\n",
    "```yaml\n",
    "opt_sim_params:\n",
    "  gs_steps: 100\n",
    "  bayesian_max_time: 4\n",
    "  bayesian_max_it: 4000\n",
    "  scenario_features:\n",
    "    - ALIM_FLOT_CU_TOT\n",
    "    - PH_ROUGHER_COND\n",
    "    - P20_MOAGEM\n",
    "  decision_variables:\n",
    "    - VAZAO_AR_ROUGHER_COND\n",
    "    - Espumante (g/t)_CD\n",
    "    - ESPUMA_ROUGHER_COND\n",
    "```\n",
    "\n",
    "Neste arquivo, definem-se os seguintes parâmetros: \n",
    "\n",
    "* `scenario_features`: o conjunto de variáveis de contexto $X$. Estas variáveis serão utilizadas para encontar 30 cenários distintos na base de dados histórica;\n",
    "* `decision_variables`: o conjunto de variáveis de decisão $V_d$;\n",
    "* Parâmetros da otimização:\n",
    "  * `gs_steps`: quantidade de valores testados para cada variável de decisão dentro do algoritmo grid-search. Deve-se levar em consideração que altos valores deste parâmetro irão resultar em tempos de execução demasiadamente longos. Deve-se considerar que o total de valores testados é $\\text{gs-steps}^m$, sendo $m$ a quantidade de variáveis de decisão. Ou seja, para 3 variáveis de decisão com 100 gs_steps, tem-se 1 milhão de combinações. Para 5 variáveis de decisão, tem-se 10 bilhões de combinações. 200 milhões de combinações é um valor alto, porém com tempo de execução razoável por volta de 10 minutos (depende da capacidade de processamento da CPU utilizada) para cada otimização; \n",
    "  * `bayesian_max_time`: tempo máximo de execução da otimização bayesiana;\n",
    "  * `bayesian_max_it`: quantidade de iterações máximas da otimização bayesiana.\n",
    "\n",
    "Após a parametrização do arquivo de configuração, pode-se executar o pipeline com `kedro run --pipeline=optimization`. O tempo de execução depende dos parâmetros estabelecidos, mas a execução dura em média ~2 horas considerando 4 minutos por otimização bayesiana (`bayesian_max_it`), 100 passos no grid-search (`gs_steps`) e 3 variáveis de decisão.\n",
    "\n",
    "A execução do pipeline resulta em um plot salvo na pasta `data/08_reporting/conc_cd_simulations_plot.png`, segue um exemplo abaixo:\n",
    "\n",
    "<img src=\"Images/exemplo_otm.png\" alt=\"Otimização\" width=\"1000\"/>\n",
    "\n",
    "Neste plot, têm-se informações acerca da otimização para cada uma das variáveis de decisão e também de seus valores observados no histórico. As duas primeiras linhas se referem ao comportamento das variáveis de decisão em relação à função objetivo para cada cenário otimizado. Neste exemplo, a função objetivo a ser maximizada foi o valor predito pelo modelo de teor de cobre no CD. A escala dos gráficos vão de 0 a 1 que se traduzem no mínimo e máximo de teor de cobre encontrado para diferentes valores da variável de decisão em cada cenário.\n",
    "\n",
    "A última linha de gráficos se refere um resumo dos valores ótimos para cada variável de decisão utilizando os otimizadores e também o observado no histórico.\n",
    "\n",
    "Estas análises são úteis para verificar a diversidade das soluções sugeridas pelo otimizador para cada uma das variáveis de decisão, ou seja, nos distintos cenários o modelo escolhe um mesmo valor para uma determinada variável de decisão ou o modelo é capaz de levar o contexto em consideração para sugerir diferentes valores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.8 <a id='section_38'></a>[Atualização do simulador MVP](#section_3)\n",
    "\n",
    "A atualização do Simulador MVP é um processo essencial para garantir que o sistema esteja alinhado com as últimas informações em termos de dados, modelos preditivos e práticas operacionais. É recomendável realizar este processo de atualização com frequência, idealmente uma vez por semana, para assegurar a relevância e eficácia contínua do simulador. Nesta seção, detalhamos as etapas chave para manter o simulador atualizado, que incluindo:\n",
    "\n",
    "- [Visão geral das pastas do WebApp](#secao_381)\n",
    "- [Implementação de novos modelos preditivos](#secao_382)\n",
    "- [Adição de novas features](#secao_383)\n",
    "- [Atualização do Tipo de Otimizador](#secao_384)\n",
    "\n",
    "#####  <a id='secao_381'></a> **Visão geral das pastas do WebApp**\n",
    "\n",
    "`00_metadata`\n",
    "\n",
    "-   Descrição: Armazena as variáveis e suas etapas no processo.\n",
    "-   Arquivos principais:\n",
    "    -   `etapas.yaml`: Dicionário contendo mapeamento das variáveis e suas etapas.\n",
    "\n",
    "`01_raw_data`\n",
    "\n",
    "-   Descrição: Contém os dados brutos inseridos pelos usuários.\n",
    "-   Arquivos principais:\n",
    "    -   `balanco_de_massas.xlsx`: Dados brutos do balanço de massas.\n",
    "    -   `blend.xlsx`: Informações relacionadas ao blend.\n",
    "    -   `carta_controle_pims.xlsx`: Dados de controle de processos PIMS.\n",
    "    -   `laboratorio.xlsx` e `laboratorio_raiox.xlsx`: Dados brutos do laboratório e raio-X.\n",
    "    -   `reagentes.xlsx`: Informações sobre os reagentes utilizados.\n",
    "\n",
    "`02_intermediate`\n",
    "\n",
    "-   Descrição: Armazena dados processados em um estágio intermediário.\n",
    "-   Arquivos principais:\n",
    "    -   `carta_controle_pims_pre.pq`: Dados pré-processados de controle PIMS.\n",
    "    -   `balanco_de_massas_pre.pq`: Dados pré-processados do balanço de massas.\n",
    "    -   `laboratorio_raiox_pre.pq` e `laboratorio_pre.pq`: Dados pré-processados do laboratório e raio-X.\n",
    "    -   `reagentes_pre.pq`: Dados pré-processados dos reagentes.\n",
    "    -   `blend_pre.pq`: Dados pré-processados de blend.\n",
    "\n",
    "`03_primary`\n",
    "\n",
    "-   Descrição: Contém dados primários processados e prontos para uso.\n",
    "-   Arquivos principais:\n",
    "    -   `merged_raw_data.pq`: Dados brutos mesclados.\n",
    "    -   `filtered_data.pq`: Dados filtrados.\n",
    "    -   `hourly_data.pq`: Dados processados em base horária.\n",
    "\n",
    "`04_models`\n",
    "\n",
    "-   Descrição: Armazena os modelos de machine learning.\n",
    "-   Arquivos principais:\n",
    "    -   `conformal_model_conc_cd_randomly.pickle`: Modelo conformal para concentração na CD.\n",
    "    -   `ebm_conc_cd_randomly.pickle`: Modelo EBM para conetração na CD.\n",
    "\n",
    "##### <a id='secao_382'></a> **Implementação de novos modelos preditivos**\n",
    "\n",
    "1.   **Seleção do Modelo Preditivo:** A escolha do modelo mais adequado para integração no simulador é um passo crucial. Esta decisão deve ser tomada após a conclusão das fases de [otimização dos hiperparâmetros dos modelos preditivos](#section_34), [análise de erro/performance](#section_36) e a subsequente [análise da simulação do otimizador com o modelo preditivo](#section_37). Estes processos asseguram que o modelo selecionado esteja não apenas tecnicamente afinado com parâmetros e hiperparâmetros otimizados, mas também alinhado com as necessidades e realidades do negócio, especialmente em relação à variação das variáveis de decisão.\n",
    "\n",
    "2.   **Implementação do Modelo Preditvo no Simulador:** Após a seleção do modelo preditivo, prossiga com a integração do modelo escolhido ao simulador. Este processo envolve copiar os modelos `ebm_conc_cd_randomly.pickle` e `conformal_model_conc_cd_randomly.pickle` encontrados em **data/06_models/** e colar no diretório **app/data/04_models** do simulador.\n",
    "\n",
    "3. **Integração dos Dados:** Para a independência de importação de planilhas ou conexão com banco de dados no WebApp, é crucial integrar os dados de maneira adequada. Isso inclui copiar o arquivo `data/03_primary/merged_raw_data.pq` para o diretório `app/data/03_primary/`, renomeando-o para `hourly_data.pq`. Esse passo garante que o WebApp tenha acesso aos dados necessários.\n",
    "\n",
    "##### <a id='secao_383'></a> **Adição de novas *features***\n",
    "\n",
    "Em casos de novas *features* no modelo preditivo implementado, ou seja,  a adição de *features* ainda não presentes no dicionário `etapas.yaml`, será necessário adicionar o nome da etapa do processo de flotação referente (caso não exista), e o nome da variável juntamente com a sua unidade de medida. Esta adição garante que as novas *features* sejam corretamente integradas e reconhecidas no sistema dinâmico do webapp (o mesmo passo a passo funciona para as variáveis de decisão). \n",
    "\n",
    "<img src=\"Images/exemplo_adicao_variavel_webapp.png\" alt=\"WebApp\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "####  <a id='secao_384'></a> **Atualização do Tipo de Otimizador**\n",
    "\n",
    "Para garantir a eficácia das simulações, pode ser necessário alternar entre diferentes otimizadores seguindo as análises . Siga as instruções abaixo para atualizar o otimizador no WebApp:\n",
    "\n",
    "1.  Configuração do Otimizador: Acesse `app\\data\\00_metadata\\etapas.yaml` e localize a seção `otimizador`. Substitua o tipo atual pelo desejado (por exemplo, de \"Bayesian\" para \"GridSearch\").\n",
    "\n",
    "2.  Aplicação das Mudanças: Salve as alterações em `etapas.yaml` e reinicie o WebApp para que a nova configuração tenha efeito.\n",
    "\n",
    "<img src=\"Images/exemplo_modificar_otimizacao_webapp.png\" alt=\"Otimizador\" width=\"1000\"/>\n",
    "\n",
    "Para utilizar o MVP com os modelos atualizados, executar o comando `streamlit run app/Sobre.py` no terminal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 <a id='section_4'></a>[Fluxo de Uso](#toc0_) [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O fluxo de uso com todas as etapas a serem executadas está definido a seguir:\n",
    "\n",
    "  - 4.1 [Processar Dados Novos](#section_41)\n",
    "  - 4.2 [Calibrar Modelo Baseline e Executar Simulação de Otimização](#section_42)\n",
    "  - 4.3 [Otimização de Hiperparâmetros do Modelo Campeão](#section_43)\n",
    "  - 4.4 [Calibrar Modelo Campeão e Executar Simulação de Otimização](#section_44)\n",
    "  - 4.5 [Comparar Resultados](#section_45)\n",
    "  - 4.6 [Atualizar Modelo no MVP (WebApp)](#section_46)\n",
    "\n",
    "\n",
    "#### 4.1  <a id='section_41'></a>[Processar Dados Novos](#section_4)\n",
    "\n",
    "* Configurar o arquivo **conf/base/parameters_data_processing.yml** e executar o pipeline:\n",
    "\n",
    "  ```bash\n",
    "  kedro run --pipeline=data_processing\n",
    "  ```\n",
    "\n",
    "#### 4.2  <a id='section_42'></a>[Calibrar Modelo Baseline e Executar Simulação de Otimização](#section_4)\n",
    "\n",
    "* Executar o notebook **Dynamic Feature.ipynb** e avaliar mudanças na dinâmica do processo.\n",
    "\n",
    "* Configurar o arquivo **conf/base/parameters_generate_models_inputs.yml** e executar o pipeline:\n",
    "\n",
    "  ```bash\n",
    "  kedro run --pipeline=generate_models_inputs\n",
    "  ```\n",
    "\n",
    "* Configurar o arquivo **conf/base/parameters_data_science.yml** e executar o pipeline:\n",
    "  \n",
    "  ```bash\n",
    "  kedro run --pipeline=data_science\n",
    "  ```\n",
    "\n",
    "* Configurar o arquivo **conf/base/parameters_optimization.yml** e executar o pipeline:\n",
    "\n",
    "  ```bash\n",
    "  kedro run --pipeline=optimization\n",
    "  ```\n",
    "\n",
    "#### 4.3  <a id='section_43'></a>[Otimização de Hiperparâmetros do Modelo Campeão](#section_4)\n",
    "\n",
    "* Configurar o arquivo **conf/base/pparameters_models_optimization.yml** e executar o pipeline:\n",
    "\n",
    "  ```bash\n",
    "  kedro run --pipeline=models_optimization\n",
    "  ```\n",
    "\n",
    "#### 4.4  <a id='section_44'></a>[Calibrar Modelo Campeão e Executar Simulação de Otimização](#section_4)\n",
    "\n",
    "* Executar o notebook **Dynamic Feature.ipynb** e avaliar mudanças na dinâmica do processo.\n",
    "\n",
    "* Configurar o arquivo **conf/base/parameters_generate_models_inputs.yml** e executar o pipeline:\n",
    "\n",
    "  ```bash\n",
    "  kedro run --pipeline=generate_models_inputs\n",
    "  ```\n",
    "\n",
    "* Configurar o arquivo **conf/base/parameters_data_science.yml** e executar o pipeline:\n",
    "  \n",
    "  ```bash\n",
    "  kedro run --pipeline=data_science\n",
    "  ```\n",
    "\n",
    "* Configurar o arquivo **conf/base/parameters_optimization.yml** e executar o pipeline:\n",
    "\n",
    "  ```bash\n",
    "  kedro run --pipeline=optimization\n",
    "  ```\n",
    "\n",
    "#### 4.5  <a id='section_45'></a>[Comparar Resultados](#section_4)\n",
    "\n",
    "* Executar notebooks e avaliar as métricas de erro.\n",
    "\n",
    "  * Modelagem-TeorCu-Cd-Baseline.ipynb\n",
    "  * Modelagem-TeorCu-Cd new pipeline.ipynb\n",
    "  * Modelagem-RM-Global-Baseline.ipynb\n",
    "  * Modelagem-RM-Global new pipeline.ipynb\n",
    "\n",
    "#### 4.6  <a id='section_46'></a>[Atualizar Modelo no MVP (WebApp)](#section_4)\n",
    "\n",
    "\n",
    "\n",
    "1. Vá até o diretório **data/06_models**.\n",
    "\n",
    "2. Localize os arquivos denominados `conformal_model_conc_cd_randomly.pickle` e `ebm_conc_cd_randomly.pickle`.\n",
    "\n",
    "3. Copie ambos os arquivos.\n",
    "\n",
    "4. Navegue até o diretório **app/data/04_models**.\n",
    "\n",
    "5. Cole os arquivos previamente mencionados neste novo diretório.\n",
    "\n",
    "6. (Opcional) Altere o arquivo **app/data/00_metadata/etapas.yml** caso o novo modelo possua *features* adicionais.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvvflotacao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
